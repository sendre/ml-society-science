\section{Nearest neighbours}
\only<presentation>{
  \begin{frame}
    \tableofcontents[ 
    currentsection, 
    hideothersubsections, 
    sectionstyle=show/shaded
    ] 
  \end{frame}
}


\begin{frame}
  \frametitle{Discriminating between diseases}
  \input{../figures/KNN3.tikz}
\end{frame}

\only<article>{
  Let's tackle the problem of discriminating between different
  disease vectors. Ideally, we'd like to have a simple test that
  tells us what ails us. One kind of test is mass spectrometry. This
  graph shows spectrometry results for two types of bacteria. There
  is plenty of variation within each type, both due to measurement
  error and due to changes in the bacterial strains. Here, we plot
  the average and maximum energies measured for about 100 different
  examples from each strain.
}


\begin{frame}
  \frametitle{Nearest neighbour: the hidden secret of machine learning}
  \input{../figures/separation1.tikz}
\end{frame}


\only<article>{ Now, is it possible to identify an unknown strain
  based on this data? Actually, this is possible. Sometimes, very
  simple algorithms work very well. One of the simplest one involves
  just measuring the distance between the decsription of a new unknown
  strain and known ones. In this visualisation, I projected the
  1300-dimensional data into a 2-dimensional space. Here you can
  clearly see that it is possible to separate the two strains. We can
  use the distance to examples VVT and BUT in order to decide the type
  of an unknown strain.  }

\begin{frame}
  \frametitle{Comparing spectral data}
  \only<1>{\input{../figures/difference1.tikz}}
  \only<presentation>{\only<2>{\input{../figures/difference2.tikz}}}
\end{frame}

\only<article>{
  The choice of distance in this kind of algorithm is important,
  particularly for very high dimensions. For something like a
  spectrogram, one idea is look at the total area of the difference
  between two spectral lines. 
}

\begin{frame}
  \frametitle{The nearest neighbour algorithm}
  \only<article>{The nearest neighbour algorithm for classification (Alg.~\ref{alg:kNN-classify}) does not include any complicated learning. Given a training dataset $D$, it returns a classification decision for any new point $x$ by simply comparing it to its closest $k$ neighbours in the dataset. It then estimates the probability $p_y$ of each class $y$ by calculating the average number of times the neighbours take the class $y$.
  }
  \begin{algorithm}[H]
    \begin{algorithmic}[1]
      \State \textbf{Input} Data $D = \{(x_1, y_1), \ldots, (x_T, y_T)\}$, $k \geq 1$,  $d : \CX \times \CX \to \Reals_+$, new point $x \in \CX$
      \State $D = \texttt{Sort}(D, d)$ \% \textsf{ Sort $D$ so that $d(x, x_i) \leq d(x, x_{i+1})$}.
      \State $p_y = \sum_{i=1}^k \ind{y_i = y} / k$ for $y \in \CY$.
      \State \textbf{Return} $\vp \defn (p_1, \ldots, p_k)$
    \end{algorithmic}
    \caption{\KNN{} Classify}
    \label{alg:kNN-classify}
  \end{algorithm}
  \begin{alertblock}{Algorithm parameters}
    \only<article>{In order to use the algorithm, we must specify some parameters, namely.}
    \begin{itemize}
    \item Neighbourhood $k \geq 1$. \only<article>{The number of neighbours to consider.}
    \item Distance $d : \CX \times \CX \to \Reals_+$. \only<article>{The function we use to determine what is a neighbour.}
    \end{itemize}
  \end{alertblock}
  \only<presentation>{
    What does the algorithm output when $k = T$?
  }
\end{frame}


\begin{frame}
  \begin{figure}[H]
    \centering
    \includegraphics[width=\fwidth]{../figures/fix_evelyn2}
    \includegraphics[width=\fwidth]{../figures/Hodges}
    \caption{The nearest neighbours algorithm was introduced by \citet{fix1951discriminatory}, who also proved consistency properties.}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Nearest neighbour: What type is the new bacterium?}
  \input{../figures/separation2.tikz}
  \only<presentation>{
    \only<2>{\newline \Large \alert{What if it a \textbf{completely different strain}?}}
  }
  \only<article>{Given that the $+$ points represent the BUT type, and the $\times$ points the VVJ type, what type of bacterium could the circle point be?}
\end{frame}

\begin{frame}
  \frametitle{Separating the model from the classification policy}
  \begin{itemize}
  \item The \KNN{} algorithm returns a model giving class probabilities for new data points.
  \item \only<article>{It is up to us to decide how to use this model to decide upon a given class. A typical decision making rule can be in the form of a policy $\pol$ that depends on what the model says. However, the simplest decision rule is to take the most likely class:}
    \only<presentation>{Deciding a class given the model}
    \[
      \pol(a \mid x) = \ind{p_a \geq p_y \forall y}, \qquad \vp = \KNN(D, k, d, x)
    \]
  \end{itemize}
\end{frame}

\only<presentation>{
  \begin{frame}
    \frametitle{Hands on with Python console}
    \hyperlink{../src/decision-problems/knn-classify.py}{\beamerbutton{KNN example}}
    %% Use knn-classify to simply demonstrate a kNN classifier.
  \end{frame}
}

\begin{frame}
  \frametitle{Discussion: Shortcomings of $k$-nearest neighbour}
  \begin{itemize}
  \item Choice of $k$ \only<article>{The larger $k$ is, the more data you take into account when making your decision. This means that you expect your classes to be more spread out.} 
  \item Choice of metric $d$. \only<article>{The metric $d$ encodes prior information you have about the structure of the data.}
  \item Representation of uncertainty. \only<article>{The predictions of kNN models are simply based on distances and counting. This might not be a very good way to represent uncertainty about class label. In particular, label probabilities should be more uncertain when we have less data.}
  \item Scaling with large amounts of data. \only<article>{A naive implementation of kNN requires the algorithm to shift through all the training data to find the $k$ nearest neighbours, suggesting a superlinear computation time. However, advanced data structures such as Cover Trees (or even KD-trees in low dimensional spaces) can be used to find the neighbours in polylog time.}
    \item Meaning of label probabilities.  \only<article>{It is best to think of \KNN{} as a \alert{model} for predicting the class of a new example from a finite set of existing classes. The model itself might be incorrect, but this should nevertheless be OK for our purposes. In particular, we might later use the model in order to derive classification rules.}
  \end{itemize}


\end{frame}

\begin{frame}
  \frametitle{Learning outcomes}
  \begin{block}{Understanding}
    \begin{itemize}
    \item How kNN works
    \item The effect of hyperameters $k, d$ for nearest neighbour.
    \item The use of kNN to classify new data.
    \end{itemize}
  \end{block}
  
  \begin{block}{Skills}
    \begin{itemize}
    \item Use a standard kNN class in python
    \item Optimise kNN hyperparameters in an unbiased manner.
    \item Calculate probabilities of class labels using kNN.
    \end{itemize}
  \end{block}

  \begin{block}{Reflection}
    \begin{itemize}
    \item When is kNN a good model?
    \item How can we deal with large amounts of data?
    \item How can we best represent uncertainty?
    \end{itemize}
  \end{block}
  
\end{frame}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "notes.tex"
%%% End:
