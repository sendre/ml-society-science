\documentclass[a4paper,twoside]{book}
\usepackage{geometry}
\usepackage[notheorems]{beamerarticle}



\mode<presentation>{
  % \useinnertheme{rectangles}
  %\useoutertheme{infolines}
  % \usecolortheme{crane}
  % \usecolortheme{rose}
}
\input{../preamble}

%\includeonly{database,privacy}
%\includeonly{experiment-design}
%\includeonly{networks}
\title{Machine learning in science and society}
\subtitle{From automated science to beneficial artificial intelligence}
\author[C. Dimitrakakis]{Christos Dimitrakakis}
\begin{document}

\maketitle
\tableofcontents

\chapter{Introduction}
\include{ml-intro}

\chapter{Simple decision problems}

\only<article>{This chapter deals with simple decision problems, whereby a decision maker (DM) makes a simple choice among many. In some of this problems the DM has to make a decision after first observing some side-information. Then the DM uses a \emph{decision rule} to assign a probability to each possible decision for each possible side-information. However, designing the decision rule is not trivial, as it relies on previously collected data. A higher-level decision includes choosing the decision rule itself. The problems of classification and regression fall within this framework. While most steps in the process can be automated and formalised, a lot of decisions are actual design choices made by humans. This creates scope for errors and misinterpretation of results.

In this chapter, we shall formalise all these simple decision problems from the point of view of statistical decision theory. The first question is, given a real world application, what type of decision problem does it map to? Then, what kind of machine learning algorithms can we use to solve it? What are the underlying assumptions and how valid are our conclusions? 
}

\include{knn}  % knn, reproducability and bootstrapping
\include{reproducibility}
\include{reproducibility-assignment}
\include{bayes} % Bayesian inference
\include{decision-problems} % decision hierarchies
\include{ann} % linear models and stochastic gradient descent
\include{naive-bayes} % naive Bayes classifiers
%\include{credit}

\chapter{Privacy}
\only<article>{
  Participating in a study always carries a risk for individuals, namely that of data disclosure. In this chapter, we first explain how simple database query methods, and show even a small number of queries to a database they can compromise the privacy of individuals. We then introduce to formal concepts of privacy protection: $k$-anonymity and differential privacy. The first is relatively simple to apply and provides some limited resistance to identification of individuals through record linkage attacks. The latter is a more general concept, and can be simple apply in some settings, while it offers information-theoretic protection to individuals. A major problem with any privacy definition and method, however is correct interpretation of the privacy concept used, and correct implementation of the algorithm used.}

\include{database} % data base access model
\include{privacy} %data bases

\chapter{Fairness}
\only<article>{
  When machine learning algorithms are applied at scale, it can be difficult to imagine
}
\include{fairness}
\include{credit}



\chapter{Recommendation systems}
\only<article>{Structured learning problems involve multiple latent variables with a complex structure. These range from clustering and spech recognition to DNA and biological and social network analysis. Since structured problems include relationships between many variables, they can be analysed using graphical models.}
\include{recommendation}
\include{clustering}
\include{networks}
\include{hmm}
\include{rnn}

\chapter{Bandit problems}
\include{bandit}
\include{experiment-design}
\chapter{Markov decision processes}
\include{mdp}
\chapter{Safety}
\include{safety}


\bibliographystyle{plainnat}
\bibliography{../bibliography}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-engine: xetex
%%% TeX-master: "notes"
%%% End:
